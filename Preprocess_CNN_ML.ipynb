{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3655,"status":"ok","timestamp":1672173594689,"user":{"displayName":"Nicolas Godron","userId":"07015768632215272870"},"user_tz":-60},"id":"MGorXt3W0rsE","outputId":"02afaced-92c6-4fce-e0d5-ce8afbc49dba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"St79nITNxPsx","outputId":"774d4f44-d022-475c-ffde-cc52381419bb","executionInfo":{"status":"ok","timestamp":1672182426828,"user_tz":-60,"elapsed":528697,"user":{"displayName":"Nicolas Godron","userId":"07015768632215272870"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["gdrive/MyDrive/Etudes/GENIOMHE/M2_EFG/Machine_Learning/Training_SetA/p000002.psv\n","gdrive/MyDrive/Etudes/GENIOMHE/M2_EFG/Machine_Learning/Training_SetA/p000003.psv\n","gdrive/MyDrive/Etudes/GENIOMHE/M2_EFG/Machine_Learning/Training_SetA/p000004.psv\n","gdrive/MyDrive/Etudes/GENIOMHE/M2_EFG/Machine_Learning/Training_SetA/p000005.psv\n","gdrive/MyDrive/Etudes/GENIOMHE/M2_EFG/Machine_Learning/Training_SetA/p000006.psv\n","HR|O2Sat|Temp|SBP|MAP|DBP|Resp|EtCO2|BaseExcess|HCO3|FiO2|pH|PaCO2|SaO2|AST|BUN|Alkalinephos|Calcium|Chloride|Creatinine|Bilirubin_direct|Glucose|Lactate|Magnesium|Phosphate|Potassium|Bilirubin_total|TroponinI|Hct|Hgb|PTT|WBC|Fibrinogen|Platelets|Age|Gender|Unit1|Unit2|HospAdmTime|ICULOS|SepsisLabel\n","NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|75.91|0|0|1|-98.6|1|0\n","61|99|36.44|124|65|43|17.5|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|75.91|0|0|1|-98.6|2|0\n","64|98|NaN|125|64|41|27|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|75.91|0|0|1|-98.6|3|0\n","56|100|NaN|123|65|41|9|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|75.91|0|0|1|-98.6|4|0\n","\n","NaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t75.91\t0\t0\t1\t-98.6\t1\t0\n","61\t99\t36.44\t124\t65\t43\t17.5\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t75.91\t0\t0\t1\t-98.6\t2\t0\n","64\t98\tNaN\t125\t64\t41\t27\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t75.91\t0\t0\t1\t-98.6\t3\t0\n","56\t100\tNaN\t123\t65\t41\t9\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t75.91\t0\t0\t1\t-98.6\t4\t0\n","66\t99\tNaN\t120\t67\t43\t23\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t75.91\t0\t0\t1\t-98.6\t5\t0\n","Number of lines in .tsv:\n","869183 gdrive/MyDrive/Etudes/GENIOMHE/M2_EFG/Machine_Learning/Training_SetA/All_patients_Training_SetA.tsv\n","Number of files (= Number of 'header' lines removed):\n","20335\n","Sum of all lines:\n","891544\n","   810496 total\n"]}],"source":["#@title Concatenating input files into one tab-separated-values file (.tsv, runs in about 5-10 minutes)\n","\n","%%bash\n","\n","dataset=\"Training_SetA\"\n","folder=\"gdrive/MyDrive/Etudes/GENIOMHE/M2_EFG/Machine_Learning/${dataset}/\"\n","\n","# This block removes potential duplicate input files in the Google Drive folder\n","ls $folder | grep '\\ (1).psv' > ${folder}twice_aint_nice.txt\n","while IFS=\"\" read -r p || [ -n \"$p\" ]\n","do\n","  rm \"${folder}${p}\"\n","  echo \"Duplicate file removed:\" ${folder}${p} \n","done < ${folder}twice_aint_nice.txt\n","\n","psv_list=$(ls ${folder}p*.psv)\n","psv_count=$(ls -alF ${folder}p*.psv | wc -l | cut -d ' ' -f 1)\n","echo $psv_list | cut -d ' ' -f 1-5 | tr \" \" \"\\n\"\n","\n","psv=\"${folder}All_patients_${dataset}.psv\"\n","tsv=\"${folder}All_patients_${dataset}.tsv\"\n","touch $psv\n","line_count=\"${folder}All_patients_${dataset}.lines\"\n","touch $line_count\n","\n","for file in ${psv_list}\n","do\n","cat ${file} >> $psv\n","wc -l $file | cut -d ' ' -f 1 >> $line_count\n","done\n","\n","head -n 5 $psv\n","\n","# Processing to remove header (Starts with HR) and switch separator from pipe to tabulation.\n","cat $psv | grep --invert-match -E 'HR' | tr \"|\" \"\\t\" > $tsv\n","\n","printf \"\\n\"\n","head -n 5 $tsv\n","\n","echo \"Number of lines in .tsv:\"\n","wc -l $tsv\n","echo \"Number of files (= Number of 'header' lines removed):\"\n","echo \"${psv_count}\"\n","echo \"Sum of all lines:\"\n","awk '{ sum += $1 } END { print sum }' $line_count\n","wc -l $psv_list | tail -n 1 # Retrieves only the last line of the word count: total\n","\n","rm $psv $line_count"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49871,"status":"ok","timestamp":1670426062271,"user":{"displayName":"Nicolas Godron","userId":"07015768632215272870"},"user_tz":-60},"id":"Ub_HJ3A4xUc4","outputId":"b4f276b2-b497-46d2-b4ba-a10208a46be7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Process is interrupted.\n"]}],"source":["#@title Testing length of all files against merged file # Do not execute - WIP\n","\n","%%bash\n","\n","## Doesn't work yet :(\n","\n","date\n","\n","# This script is to check proper function of earlier script\n","\n","dataset=\"Training_SetA\"\n","folder=\"drive/MyDrive/Etudes/GENIOMHE/M2_EFG/Machine_Learning/${dataset}/\"\n","\n","tsv=\"${folder}All_patients_${dataset}.tsv\"\n","\n","linecount=0\n","filecount=0\n","\n","for i in $(ls ${folder}/*.psv)\n","do\n","  lines= $(wc -l $i | cut -d' ' -f1)\n","  # linecount= $((linecount + lines))\n","  # filecount= $((filecount + 1))\n","  echo $lines\n","done\n","\n","total= $((linecount - filecount))\n","\n","printf \"All patients line:\\n\"\n","wc -l $tsv | cut -d' ' -f1\n","\n","printf \"Sum of counts:\\n\"\n","printf $linecount\n","printf \"- \\n\"\n","printf $filecount\n","printf \"= \\n\"\n","printf $total\n","\n","printf \"\\n\"\n","date"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37813,"status":"ok","timestamp":1672182645975,"user":{"displayName":"Nicolas Godron","userId":"07015768632215272870"},"user_tz":-60},"id":"nsCGHW4s3W8D","outputId":"5799e643-6fe2-4f67-cded-67db8e6504e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Medians (file called All_patients_Training_SetA.median): \n","84\n","98\n","37.06\n","118\n","77\n","58.5\n","18\n","0\n","24\n","0.5\n","7.38\n","40\n","97\n","57\n","18\n","79\n","8.3\n","106\n","0.9\n","1.4\n","124\n","1.8\n","2\n","3.4\n","4.1\n","0.9\n","4.25\n","30.2\n","10.4\n","32.4\n","10.8\n","248\n","181\n","65.27\n","1\n","1\n","0\n","-2.54\n","21\n","0\n","84\n","98\n","37.06\n","118\n","77\n","58.5\n","18\n","0\n","24\n","0.5\n","7.38\n","40\n","97\n","57\n","18\n","79\n","8.3\n","106\n","0.9\n","1.4\n","124\n","1.8\n","2\n","3.4\n","4.1\n","0.9\n","4.25\n","30.2\n","10.4\n","32.4\n","10.8\n","248\n","181\n","65.27\n","1\n","1\n","0\n","-2.54\n","21\n","0\n"]},{"output_type":"stream","name":"stderr","text":["rm: cannot remove 'output': No such file or directory\n","rm: cannot remove 'output_bis': No such file or directory\n"]}],"source":["#@title Getting medians from full datasets\n","\n","%%bash\n","\n","# This script is to compute median of each column from every patient's data\n","\n","dataset=\"Training_SetA\"\n","folder=\"gdrive/MyDrive/Etudes/GENIOMHE/M2_EFG/Machine_Learning/${dataset}/\"\n","\n","tsv=\"${folder}All_patients_${dataset}.tsv\"\n","\n","median_file=\"${folder}All_patients_${dataset}.median.txt\"\n","log_file=\"${folder}All_patients_${dataset}.log\"\n","\n","n_columns=$(awk '{print NF}' $tsv | sort -nu | tail -n 1)\n","\n","printf \"Count of columns: $n_columns\\n\" > $log_file\n","printf \"Count of lines: $(wc -l $tsv | cut -d' ' -f1)\\n\\n\" >> $log_file\n","\n","\n","for i in $(seq $n_columns)\n","do\n","  cut -d\"\t\" -f ${i} $tsv > tmp\n","  grep --invert-match -E 'NaN' tmp > tmp1 # To remove \"NaN\" lines in each column\n","  if [ -s tmp1 ]; then\n","        # The file is not-empty.\n","  \n","  # Computes medians\n","\n","  sort -n tmp1 | awk ' { a[i++]=$1; } \\\n","     END { x=int((i+1)/2); if (x < (i+1)/2) print (a[x-1]+a[x])/2; else print a[x-1]; }' >> $median_file\n","  \n","  nonempty=$(wc -l tmp1 | cut -d' ' -f1)\n","  printf \"Column $i has $nonempty non-empty values\\n\" >> $log_file\n","  \n","  median_col=$(tail -n 1 $median_file)\n","  printf \"Median value: ${median_col}\\n\\n\" >> $log_file\n","  \n","  else\n","        # The file is empty.\n","  printf \"Column $i is empty for all patients and should be removed from all patients\\n\\n\" >> $log_file\n","  fi\n","done\n","\n","printf \"Medians (file called All_patients_${dataset}.median.txt): \\n\"\n","cat $median_file\n","\n","# From https://stackoverflow.com/questions/6166375/median-of-column-with-awk\n","\n","# Approximate median column 1\n","# sort -n file | awk ' { a[i++]=$1; } END { print a[int(i/2)]; }'\n","\n","# True median column 1\n","# sort -n file | awk ' { a[i++]=$1; }\n","#    END { x=int((i+1)/2); if (x < (i+1)/2) print (a[x-1]+a[x])/2; else print a[x-1]; }'"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"17_GMwT9pupDaOuKu2cQCcvJfUim-YrcP","authorship_tag":"ABX9TyMjXjFpWVDw66GsQvUKMtm+"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}